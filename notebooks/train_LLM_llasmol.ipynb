{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip install transformers==4.38.1\n",
    "# ! pip install rdkit==2023.9.4\n",
    "# ! pip install accelerate==0.27.2\n",
    "# ! pip install flash-attn\n",
    "# ! pip install -q -U bitsandbytes\n",
    "# ! pip install datasets\n",
    "# ! pip install loralib\n",
    "# ! pip install git+https://github.com/huggingface/peft.git\n",
    "# ! pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random, pickle, json, os\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import bitsandbytes as bnb\n",
    "from peft import PeftModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from math import sqrt\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "import sys\n",
    "sys.path.append('../credentials/')\n",
    "from HF_credentials import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', token=HF_CREDENTIALS, model_max_length=256, add_prefix_space=False)\n",
    "llm_tokenizer.pad_token = llm_tokenizer.eos_token\n",
    "llm_tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = [\n",
    "  {\"role\": \"user\", \"content\": \"\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"\"}\n",
    "]\n",
    "\n",
    "llm_tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bbbp_prompts = [\n",
    "    \"Does the compound <SMILES> {smiles} </SMILES> have good blood-brain barrier permeability (BBBP)?\",\n",
    "    \"Is <SMILES> {smiles} </SMILES> BBBP positive?\",\n",
    "    \"Is blood-brain barrier permeability a property of <SMILES> {smiles} </SMILES>?\",\n",
    "    \"Is BBBP a property of <SMILES> {smiles} </SMILES>?\",\n",
    "    \"Does <SMILES> {smiles} </SMILES> have high BBBP?\",\n",
    "]\n",
    "bbbp_responses = [\n",
    "    \"<BOOLEAN> {res} </BOOLEAN>\"\n",
    "]\n",
    "\n",
    "clintox_prompts = [\n",
    "    \"Is <SMILES> {smiles} </SMILES> toxic?\",\n",
    "    \"Does <SMILES> {smiles} </SMILES> have high toxicity?\",\n",
    "    \"Is this compound toxic: <SMILES> {smiles} </SMILES>?\",\n",
    "    \"Is this molecule toxic to human: <SMILES> {smiles} </SMILES>?\",\n",
    "    \"Is <SMILES> {smiles} </SMILES> toxic to human?\",\n",
    "]\n",
    "clintox_responses = [\n",
    "    \"<BOOLEAN> {res} </BOOLEAN>\"\n",
    "]\n",
    "\n",
    "esol_prompts = [\n",
    "    \"Estimate the solubility of this molecule: <SMILES> {smiles} </SMILES>.\",\n",
    "    # \"How soluble is <SMILES> {smiles} </SMILES>?\",\n",
    "    \"Please predict the log solubility of <SMILES> {smiles} </SMILES>.\",\n",
    "    \"<SMILES> {smiles} </SMILES> - what is its solubility?\",\n",
    "    \"Can you predict the solubility of this molecule: <SMILES> {smiles} </SMILES>?\",\n",
    "    \"Calculate the log solubility of <SMILES> {smiles} </SMILES>.\",\n",
    "    \"Estimate the solubility of <SMILES> {smiles} </SMILES>.\",\n",
    "    \"What is the solubility of this compound: <SMILES> {smiles} </SMILES>?\",\n",
    "    \"What is the solubility of <SMILES> {smiles} </SMILES>?\",\n",
    "    \"How much is the log solubility of <SMILES> {smiles} </SMILES>?\"\n",
    "]\n",
    "esol_responses = [\n",
    "    \"Its log solubility is <NUMBER> {res} </NUMBER> mol/L\"\n",
    "]\n",
    "\n",
    "hiv_prompts = [\n",
    "    \"Can <SMILES> {smiles} </SMILES> serve as an inhibitor of HIV replication?\",\n",
    "    \"Can <SMILES> {smiles} </SMILES> inhibit HIV replication?\",\n",
    "    \"Is <SMILES> {smiles} </SMILES> an inhibitor of HIV replication?\",\n",
    "    \"Can the following molecule serve as an inhibitor of HIV replication: <SMILES> {smiles} </SMILES>?\",\n",
    "    \"<SMILES> {smiles} </SMILES> - Is this compound an inhibitor of HIV replication?\"\n",
    "]\n",
    "hiv_responses = [\n",
    "    \"<BOOLEAN> {res} </BOOLEAN>\"\n",
    "]\n",
    "\n",
    "lipo_prompts = [\n",
    "    \"Predict the octanol/water distribution coefficient logD under the circumstances of pH 7.4 for <SMILES> {smiles} </SMILES>.\",\n",
    "    \"Estimate the logD coefficient at pH=7.4 for the compound <SMILES> {smiles} </SMILES>.\",\n",
    "    \"For <SMILES> {smiles} </SMILES>, calculate its logD coefficient at pH=7.4.\",\n",
    "    \"Calculate the octanol/water distribution coefficient logD at pH=7.4 for this compound: <SMILES> {smiles} </SMILES>.\",\n",
    "    \"At pH of 7.4, approximate the logD coefficient for <SMILES> {smiles} </SMILES>.\"\n",
    "]\n",
    "lipo_responses = [\n",
    "    \"<NUMBER> {res} </NUMBER>\"\n",
    "]\n",
    "\n",
    "sider_prompts = [\n",
    "    \"Are there any known side effects of <SMILES> {smiles} </SMILES> affecting the heart?\",\n",
    "    \"Are there any known heart-related side effects of <SMILES> {smiles} </SMILES>?\",\n",
    "    \"Can <SMILES> {smiles} </SMILES> cause vascular disorders side effects?\",\n",
    "    \"Does <SMILES> {smiles} </SMILES> have any side effects that affect the heart?\",\n",
    "    \"Does <SMILES> {smiles} </SMILES> exhibit any heart-related side effects?\",\n",
    "    \"Does <SMILES> {smiles} </SMILES> associate with any heart-related side effects?\"\n",
    "]\n",
    "sider_reponses = [\n",
    "    \"<BOOLEAN> {res} </BOOLEAN>\"\n",
    "]\n",
    "\n",
    "def prompt_sample(task: str):    \n",
    "    if task == \"bbbp\":\n",
    "        return random.choice(bbbp_prompts), random.choice(bbbp_responses)\n",
    "    elif task == \"clintox\":\n",
    "        return random.choice(clintox_prompts), random.choice(clintox_responses)\n",
    "    elif task == \"esol\":\n",
    "        return random.choice(esol_prompts), random.choice(esol_responses)\n",
    "    elif task == \"hiv\":\n",
    "        return random.choice(hiv_prompts), random.choice(hiv_responses)\n",
    "    elif task == \"lipo\":\n",
    "        return random.choice(lipo_prompts), random.choice(lipo_responses)\n",
    "    elif task == \"sider\":\n",
    "        return random.choice(sider_prompts), random.choice(sider_reponses)\n",
    "    else:\n",
    "        raise ValueError(f\"Unrecognized task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flexible_datasets(split='train'):\n",
    "    conversations = [] # [(task, SMILES, output),...]\n",
    "    input_smiles = []\n",
    "    \n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-bbbp.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            conversations.append((\"bbbp\", txt['input'], txt['output']))\n",
    "            input_smiles.append(txt['input'])\n",
    "    \n",
    "    print(conversations[-1])\n",
    "    \n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-clintox.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            conversations.append((\"clintox\", txt['input'], txt['output']))\n",
    "            input_smiles.append(txt['input'])\n",
    "    \n",
    "    print(conversations[-1])\n",
    "    \n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-esol.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            conversations.append((\"esol\", txt['input'], txt['output']))\n",
    "            input_smiles.append(txt['input'])\n",
    "    \n",
    "    print(conversations[-1])\n",
    "    \n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-hiv.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            conversations.append((\"hiv\", txt['input'], txt['output']))\n",
    "            input_smiles.append(txt['input'])\n",
    "    \n",
    "    print(conversations[-1])\n",
    "    \n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-lipo.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            conversations.append((\"lipo\", txt['input'], txt['output']))\n",
    "            input_smiles.append(txt['input'])\n",
    "    \n",
    "    print(conversations[-1])\n",
    "    \n",
    "    with open(f'../data/LlaSMol/{split}/property_prediction-sider.jsonl', 'r') as f:\n",
    "        for line in f:\n",
    "            txt = json.loads(line)\n",
    "            conversations.append((\"sider\", txt['input'], txt['output']['Vascular disorders']))\n",
    "            input_smiles.append(txt['input'])\n",
    "    \n",
    "    print(conversations[-1])\n",
    "    print(len(conversations))\n",
    "    \n",
    "    return conversations, input_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Train:')\n",
    "train_conversations, train_input_smiles = create_flexible_datasets('train')\n",
    "print('Val:')\n",
    "val_conversations, val_input_smiles = create_flexible_datasets('val')\n",
    "print('Test:')\n",
    "test_conversations, test_input_smiles = create_flexible_datasets('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VariableCombinedDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        smiles_list,\n",
    "        conversations,\n",
    "        encoder_tokenizer,\n",
    "        llm_tokenizer,\n",
    "        max_length=256,\n",
    "        eval_mode=False\n",
    "        ):\n",
    "        self.smiles_list = smiles_list\n",
    "        self.conversations = conversations\n",
    "        self.encoder_tokenizer = encoder_tokenizer\n",
    "        self.llm_tokenizer = llm_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.eval_mode = eval_mode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.smiles_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        smiles = self.smiles_list[idx]\n",
    "        smiles_encoding = self.encoder_tokenizer(smiles, return_tensors='pt', truncation=True, padding='max_length', max_length=self.max_length)\n",
    "        \n",
    "        chat = [\n",
    "            {\"role\": \"user\", \"content\": \"\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"\"}\n",
    "        ]\n",
    "        \n",
    "        task, input_, output_ = self.conversations[idx]\n",
    "        prompt_format, response_format = prompt_sample(task)\n",
    "        prompt = prompt_format.format(smiles=input_)\n",
    "        response = response_format.format(res=output_)\n",
    "        chat[0]['content'] = prompt\n",
    "        if not self.eval_mode:\n",
    "            chat[1]['content'] = response\n",
    "        conversation = self.llm_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "        conversation_tokenized = self.llm_tokenizer(conversation, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt', add_special_tokens=False)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            return {key: tensor[0].to('cuda') for key, tensor in smiles_encoding.items()}, conversation_tokenized.to('cuda')\n",
    "        else:\n",
    "            return {key: tensor[0] for key, tensor in smiles_encoding.items()}, conversation_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load tokenizers\n",
    "chemberta_tokenizer = AutoTokenizer.from_pretrained('DeepChem/ChemBERTa-77M-MTR')\n",
    "mistral_tokenizer = AutoTokenizer.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', add_prefix_space=False)\n",
    "mistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n",
    "mistral_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Create combined dataset\n",
    "train_dataset = VariableCombinedDataset(train_input_smiles, train_conversations, chemberta_tokenizer, mistral_tokenizer)\n",
    "val_dataset = VariableCombinedDataset(val_input_smiles, val_conversations, chemberta_tokenizer, mistral_tokenizer, eval_mode=True)\n",
    "test_dataset = VariableCombinedDataset(test_input_smiles, test_conversations, chemberta_tokenizer, mistral_tokenizer, eval_mode=True)\n",
    "\n",
    "# Define DataLoader\n",
    "batch_size = 3\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x, y = next(iter(combined_loader))\n",
    "\n",
    "# mol_encoder = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\")\n",
    "# llm_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2',\n",
    "#             torch_dtype=torch.bfloat16,\n",
    "#             # quantization_config=bnb_config,\n",
    "#             device_map=\"auto\",\n",
    "#             token=HF_CREDENTIALS\n",
    "# )\n",
    "\n",
    "# mol_encoder(**x)['last_hidden_state'];\n",
    "# llm_model.model.embed_tokens(y['input_ids'].to('cuda'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MolEncoderLLMPipeline(nn.Module):\n",
    "    def __init__(self, lora_rank=32, lora_alpha=64):\n",
    "        super().__init__()\n",
    "        # Load molecule encoder\n",
    "        self.mol_encoder = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-77M-MTR\", torch_dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "        # UNCOMMENT TO BRING DOWN FROM 15GB TO 7GB\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit= True,\n",
    "            bnb_4bit_quant_type= \"nf4\",\n",
    "            bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "            bnb_4bit_use_double_quant= False,\n",
    "        )\n",
    "        self.llm_config = AutoConfig.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2', token=HF_CREDENTIALS)\n",
    "        self.llm_model = AutoModelForCausalLM.from_pretrained('mistralai/Mistral-7B-Instruct-v0.2',\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            quantization_config=bnb_config,\n",
    "            token=HF_CREDENTIALS\n",
    "        )#.to('cuda')\n",
    "\n",
    "        self.llm_model.config.use_cache = False\n",
    "        self.llm_model.config.pretraining_tp = 1\n",
    "\n",
    "        # Initialize LoRA layers for Mistral\n",
    "        self.lora_config = LoraConfig(\n",
    "            r=lora_rank,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=0.05,\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"down_proj\", \"up_proj\"],\n",
    "            bias=\"none\",\n",
    "            task_type=\"CAUSAL_LM\",\n",
    "        )\n",
    "\n",
    "        # Freeze encoder and LLM weights\n",
    "        for param in self.mol_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.linear_project = nn.Linear(self.mol_encoder.config.hidden_size, self.llm_config.hidden_size, dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "        # Apply LoRA modification\n",
    "        self.llm_model = get_peft_model(self.llm_model, self.lora_config)\n",
    "        self.llm_model.to('cuda')\n",
    "        \n",
    "    def get_embeddings(self, smiles_tokens, text_tokens):\n",
    "        # Encoder forward pass / Get SMILES embeddings\n",
    "        smiles_tokens = {k: v.to('cuda') for k, v in smiles_tokens.items()}\n",
    "        text_tokens = {k: v.to('cuda') for k, v in text_tokens.items()}\n",
    "\n",
    "        mol_encoder_output = self.mol_encoder(**smiles_tokens)\n",
    "        smiles_embedding = mol_encoder_output['last_hidden_state'][:,0,:] # torch.Size([batch, max_length, 384])\n",
    "        smiles_projection = self.linear_project(smiles_embedding).unsqueeze(1)#.to('cuda')\n",
    "        # print(smiles_projection.shape)\n",
    "\n",
    "        # Get embeddings from LLM for the question\n",
    "        embedding_layer = self.llm_model.model.model.embed_tokens\n",
    "        llm_embeddings = embedding_layer(text_tokens['input_ids']).squeeze(1)#.to('cuda') # torch.Size([batch, max_length, 4096])\n",
    "        # print(llm_embeddings.shape)\n",
    "\n",
    "        # Concatenate encoder and LLM embeddings\n",
    "        combined_embeddings = torch.cat((smiles_projection, llm_embeddings), dim=1)#.to('cuda')\n",
    "        \n",
    "        return combined_embeddings\n",
    "\n",
    "\n",
    "    def forward(self, smiles_tokens, text_tokens):\n",
    "\n",
    "        # Encoder forward pass / Get SMILES embeddings\n",
    "        smiles_tokens = {k: v.to('cuda') for k, v in smiles_tokens.items()}\n",
    "        text_tokens = {k: v.to('cuda') for k, v in text_tokens.items()}\n",
    "\n",
    "        mol_encoder_output = self.mol_encoder(**smiles_tokens)\n",
    "        smiles_embedding = mol_encoder_output['last_hidden_state'][:,0,:] # torch.Size([batch, max_length, 384])\n",
    "        smiles_projection = self.linear_project(smiles_embedding).unsqueeze(1)#.to('cuda')\n",
    "        # print(smiles_projection.shape)\n",
    "\n",
    "        # Get embeddings from LLM for the question\n",
    "        embedding_layer = self.llm_model.model.model.embed_tokens\n",
    "        llm_embeddings = embedding_layer(text_tokens['input_ids']).squeeze(1)#.to('cuda') # torch.Size([batch, max_length, 4096])\n",
    "        # print(llm_embeddings.shape)\n",
    "\n",
    "        # Concatenate encoder and LLM embeddings\n",
    "        combined_embeddings = torch.cat((smiles_projection, llm_embeddings), dim=1)#.to('cuda')\n",
    "\n",
    "        # Custom attention mask\n",
    "        # attention_mask = torch.zeros(smiles_projection.shape[0], combined_embeddings.shape[1], combined_embeddings.shape[1], device='cuda')\n",
    "        # attention_mask[:, 0, 0] = 1 # SMILES mask for itself\n",
    "        # for i in range(1, combined_embeddings.shape[1]):\n",
    "        #     attention_mask[:, i, 0:i+1] = 1 # From SMILES to current token (inclusive)\n",
    "\n",
    "        attention_mask = torch.full((smiles_projection.shape[0], combined_embeddings.shape[1], combined_embeddings.shape[1]), -float('inf'), device='cuda')\n",
    "        attention_mask[:, 0, 0] = 0 # SMILES mask for itself\n",
    "        for i in range(1, combined_embeddings.shape[1]):\n",
    "            attention_mask[:, i, 0:i+1] = 0 # From SMILES to current token (inclusive)\n",
    "\n",
    "        # attention_mask = attention_mask.unsqueeze(1)\n",
    "        # print(attention_mask.shape)\n",
    "\n",
    "        # Pass through Mistral's transformer layers with LoRA adjustments\n",
    "        output = self.llm_model(inputs_embeds=combined_embeddings, attention_mask=attention_mask)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def generate(self, smiles_tokens, text_tokens):\n",
    "\n",
    "        combined_embeddings = self.get_embeddings(smiles_tokens, text_tokens)\n",
    "\n",
    "        # Custom attention mask\n",
    "        ones = torch.ones((text_tokens['attention_mask'].shape[0], 1, 1), dtype=text_tokens['attention_mask'].dtype).to('cuda')\n",
    "        attention_mask = torch.cat((ones, text_tokens['attention_mask']), dim=-1).squeeze(1)\n",
    "        # print(attention_mask.shape)\n",
    "\n",
    "        # Generate\n",
    "        output = self.llm_model.generate(inputs_embeds=combined_embeddings, attention_mask=attention_mask, max_new_tokens=64)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = MolEncoderLLMPipeline(lora_rank=8, lora_alpha=8).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.llm_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y = next(iter(train_loader))\n",
    "# model(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval();\n",
    "# x,y = next(iter(val_loader))\n",
    "# ids = model.generate(x,y)\n",
    "# pred_sentences = [mistral_tokenizer.decode(x, skip_special_tokens=False) for x in ids.tolist()]\n",
    "# pred_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_answer(text, is_boolean):\n",
    "#     if is_boolean:\n",
    "#         start = text.find('<BOOLEAN>') + len('<BOOLEAN>')\n",
    "#         end = text.find('</BOOLEAN>')\n",
    "#         return text[start:end].strip()\n",
    "#     else:\n",
    "#         start = text.find('<NUMBER>') + len('<NUMBER>')\n",
    "#         end = text.find('</NUMBER>')\n",
    "#         return text[start:end].strip()\n",
    "\n",
    "# def get_answer(true_sentence, pred_sentence):\n",
    "#     try:\n",
    "#         true_answer = true_sentence.split('[/INST]')[1]\n",
    "#         # pred_answer = pred_sentence.split('[/INST]')[1]\n",
    "\n",
    "#         if 'BOOLEAN' in true_answer:\n",
    "#             y_true = parse_answer(true_answer, True)\n",
    "#             y_pred = parse_answer(pred_answer, True)\n",
    "#             return y_true, y_pred\n",
    "\n",
    "#         elif 'NUMBER' in true_answer:\n",
    "#             y_true = parse_answer(true_answer, False)\n",
    "#             y_pred = parse_answer(pred_answer, False)\n",
    "#             return y_true, y_pred\n",
    "#     except:\n",
    "#         return 'wrong', 'wrong'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = bnb.optim.AdamW8bit(model.parameters(), lr=1e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=mistral_tokenizer.pad_token_id)\n",
    "\n",
    "# Define the total number of training steps and the number of warmup steps\n",
    "epochs = 10\n",
    "total_steps = len(test_loader) * epochs\n",
    "warmup_steps = 1000\n",
    "\n",
    "accumulation_steps = 32\n",
    "\n",
    "# Create the learning rate scheduler\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    tprog = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, batch in tprog:\n",
    "        model.train();\n",
    "        smiles_data, conversation_data = batch\n",
    "\n",
    "        # Forward pass\n",
    "        with autocast():\n",
    "            output = model(smiles_data, conversation_data)\n",
    "            logits = output.logits[:, 1:, :]\n",
    "\n",
    "            # Prepare labels\n",
    "            labels = conversation_data['input_ids'].squeeze(1)\n",
    "            labels = torch.cat([labels[:, 1:], labels.new_full((labels.size(0), 1), mistral_tokenizer.pad_token_id)], dim=1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits.reshape(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        # Backward and accumulate gradients\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        tprog.set_description(f'train step loss: {loss.item():.4f}')\n",
    "\n",
    "        if (i+1) % accumulation_steps == 0:  # Step the optimizer every accumulation_steps\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step the scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Clean\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # # Validation step\n",
    "        # if (i % 5000 == 0) & (i == 0):\n",
    "        #     with torch.no_grad():\n",
    "        #         model.eval();\n",
    "\n",
    "        #         categories = [\"BBBP\", \"side effects\", \"logD\", \"solubility\", \"toxic\", \"HIV\"]\n",
    "        #         preds, invalid_count, trues = {cat: [] for cat in categories}, {cat: 0 for cat in categories}, {cat: [] for cat in categories}\n",
    "\n",
    "        #         def convert_to_boolean(input_string):\n",
    "        #             return True if input_string == 'Yes' else False if input_string == 'No' else None\n",
    "\n",
    "        #         val_dataset = VariableCombinedDataset(val_input_smiles[:100], val_conversations[:100], chemberta_tokenizer, mistral_tokenizer)\n",
    "        #         val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "        #         k = 0\n",
    "        #         for batch, true_sentence in zip(val_loader, val_conversations):\n",
    "        #             # Predict\n",
    "        #             smiles_data, conversation_data = batch\n",
    "        #             output_ids = model.generate(smiles_data, conversation_data)\n",
    "        #             pred_sentence = mistral_tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=False)\n",
    "        #             if k==0:\n",
    "        #                 print(pred_sentence)\n",
    "        #                 k=1\n",
    "        #             y_true, y_pred = get_answer(true_sentence, pred_sentence)\n",
    "        #             for category in categories:\n",
    "        #                 if category in true_sentence:\n",
    "        #                     if category in [\"BBBP\", \"side effects\", \"toxic\", \"HIV\"]:  # binary categories\n",
    "        #                         if y_pred in ['Yes', 'No']:\n",
    "        #                             preds[category].append(convert_to_boolean(y_pred))\n",
    "        #                             trues[category].append(convert_to_boolean(y_true))\n",
    "        #                         else:\n",
    "        #                             invalid_count[category] += 1\n",
    "        #                     else:  # continuous categories\n",
    "        #                         try:\n",
    "        #                             preds[category].append(float(y_pred))\n",
    "        #                             trues[category].append(float(y_true))\n",
    "        #                         except:\n",
    "        #                             invalid_count[category] += 1\n",
    "        #                             continue\n",
    "\n",
    "        #         for key in preds:\n",
    "        #             if len(preds[key]) > 0:  # to avoid division by zero\n",
    "        #                 if key in [\"BBBP\", \"side effects\", \"toxic\", \"HIV\"]:  # binary categories\n",
    "        #                     accuracy = accuracy_score(trues[key], preds[key])\n",
    "        #                     print(f'{key} accuracy: {accuracy:.4f}')\n",
    "        #                 else:  # continuous categories\n",
    "        #                     rmse = sqrt(mean_squared_error(trues[key], preds[key]))\n",
    "        #                     print(f'{key} RMSE: {rmse:.4f}')\n",
    "        #         print('Invalid count:')\n",
    "        #         print(invalid_count)\n",
    "\n",
    "        #         # Clean\n",
    "        #         gc.collect()\n",
    "        #         torch.cuda.empty_cache()\n",
    "\n",
    "            # Save the model\n",
    "            torch.save(model.state_dict(), f\"output/model_{epoch}_{i}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer(text, is_boolean):\n",
    "    if is_boolean:\n",
    "        start = text.find('<BOOLEAN>') + len('<BOOLEAN>')\n",
    "        end = text.find('</BOOLEAN>')\n",
    "        return text[start:end].strip()\n",
    "    else:\n",
    "        start = text.find('<NUMBER>') + len('<NUMBER>')\n",
    "        end = text.find('</NUMBER>')\n",
    "        return text[start:end].strip()\n",
    "\n",
    "true_sentence = ('bbbp',\n",
    " 'CC1=C[C@@H]2[C@H]([C@@H](O)C[C@@]3(C)[C@H]2C[C@@H](C)[C@]3(O)C(=O)COC(=O)C2=CC=CC(S(=O)(=O)O)=C2)[C@@]2(C)CC3=C(C=C12)N(C1=CC=CC=C1)N=C3',\n",
    " 'Yes')\n",
    "\n",
    "pred_sentence = '<BOOLEAN> haha </BOOLEAN>'\n",
    "\n",
    "y_true = true_sentence[-1]\n",
    "if y_true in ['Yes', 'No']:\n",
    "    y_pred = parse_answer(pred_sentence, True)\n",
    "else:\n",
    "    y_pred = parse_answer(pred_sentence, False)\n",
    "    \n",
    "categories = [\"bbbp\", \"side effects\", \"logd\", \"solubility\", \"toxic\", \"hiv\"]\n",
    "for category in categories:\n",
    "    if category in true_sentence[0]:\n",
    "        print('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true, y_pred = get_answer(true_sentence[-1], pred_sentence)\n",
    "# for category in categories:\n",
    "#     if category in true_sentence[0]:\n",
    "#         print(category)\n",
    "#         if category in [\"bbbp\", \"side effects\", \"toxic\", \"hiv\"]:  # binary categories\n",
    "#             if y_pred in ['Yes', 'No']:\n",
    "#                 print(y_pred)\n",
    "#                 preds[category].append(convert_to_boolean(y_pred))\n",
    "#                 trues[category].append(convert_to_boolean(y_true))\n",
    "#             else:\n",
    "#                 invalid_count[category] += 1\n",
    "#         else:  # continuous categories\n",
    "#             try:\n",
    "#                 preds[category].append(float(y_pred))\n",
    "#                 trues[category].append(float(y_true))\n",
    "#             except:\n",
    "#                 invalid_count[category] += 1\n",
    "#                 continue\n",
    "\n",
    "# for key in preds:\n",
    "#     if len(preds[key]) > 0:  # to avoid division by zero\n",
    "#         if key in [\"bbbp\", \"side effects\", \"toxic\", \"hiv\"]:  # binary categories\n",
    "#             accuracy = accuracy_score(trues[key], preds[key])\n",
    "#             print(f'{key} accuracy: {accuracy:.4f}')\n",
    "#         else:  # continuous categories\n",
    "#             rmse = sqrt(mean_squared_error(trues[key], preds[key]))\n",
    "#             print(f'{key} RMSE: {rmse:.4f}')\n",
    "# print('Invalid count:')\n",
    "# print(invalid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.llm_model.config.use_cache = True\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    categories = [\"BBBP\", \"side effects\", \"logD\", \"soluble\", \"toxic\", \"HIV\"]\n",
    "    preds, invalid_count, trues = {cat: [] for cat in categories}, {cat: 0 for cat in categories}, {cat: [] for cat in categories}\n",
    "\n",
    "    for batch, true_sentence in zip(test_loader, test_conversations):\n",
    "        # Predict\n",
    "        smiles_data, conversation_data = batch\n",
    "        output_ids = model.generate(smiles_data, conversation_data)\n",
    "        pred_sentence = mistral_tokenizer.decode(output_ids.tolist()[0], skip_special_tokens=False)\n",
    "        y_true, y_pred = get_answer(true_sentence, pred_sentence)\n",
    "        for category in categories:\n",
    "            if category in true_sentence:\n",
    "                if category in [\"BBBP\", \"side effects\", \"toxic\", \"HIV\"]:  # binary categories\n",
    "                    if y_pred in ['Yes', 'No']:\n",
    "                        preds[category].append(convert_to_boolean(y_pred))\n",
    "                        trues[category].append(convert_to_boolean(y_true))\n",
    "                    else:\n",
    "                        invalid_count[category] += 1\n",
    "                else:  # continuous categories\n",
    "                    try:\n",
    "                        preds[category].append(float(y_pred))\n",
    "                        trues[category].append(float(y_true))\n",
    "                    except:\n",
    "                        invalid_count[category] += 1\n",
    "\n",
    "    for key in preds:\n",
    "        if len(preds[key]) > 0:  # to avoid division by zero\n",
    "            if key in [\"BBBP\", \"side effects\", \"toxic\", \"HIV\"]:  # binary categories\n",
    "                accuracy = accuracy_score(trues[key], preds[key])\n",
    "                print(f'{key} accuracy: {accuracy:.4f}')\n",
    "            else:  # continuous categories\n",
    "                rmse = sqrt(mean_squared_error(trues[key], preds[key]))\n",
    "                print(f'{key} RMSE: {rmse:.4f}')\n",
    "    print('Invalid count:')\n",
    "    print(invalid_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
